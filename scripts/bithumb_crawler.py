"""
Airflow ì „ìš© ë¹—ì¸ FAQ í¬ë¡¤ë§ ëª¨ë“ˆ
Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ Cloudflare ë³´í˜¸ë¥¼ ìš°íšŒí•©ë‹ˆë‹¤.
appê³¼ ì™„ì „íˆ ë¶„ë¦¬ëœ ë…ë¦½ì ì¸ ëª¨ë“ˆ
"""
import asyncio
import logging
from typing import List, Dict, Optional, Set, TYPE_CHECKING
import re
from bs4 import BeautifulSoup
from datetime import datetime

# Playwright ì„¤ì •
try:
    from playwright.async_api import async_playwright, Browser, Page
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    # íƒ€ì… íŒíŠ¸ë¥¼ ìœ„í•œ ë”ë¯¸ í´ë˜ìŠ¤
    if TYPE_CHECKING:
        from playwright.async_api import Page
    else:
        # ëŸ°íƒ€ì„ì—ì„œëŠ” Any íƒ€ì… ì‚¬ìš©
        from typing import Any
        Page = Any  # type: ignore
    logging.warning("Playwrightê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# Zendesk Help Center ì„¤ì •
BASE_URL = "https://support.bithumb.com"
LOCALE = "ko"
HELP_CENTER_BASE = f"{BASE_URL}/hc/{LOCALE}"

logger = logging.getLogger(__name__)


def extract_images_from_element(soup: BeautifulSoup) -> List[Dict]:
    """ìš”ì†Œì—ì„œ ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ"""
    images = []
    
    if not soup:
        return images
    
    img_tags = soup.find_all('img')
    
    for img in img_tags:
        img_info = {}
        
        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
        img_url = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
        if img_url:
            if img_url.startswith('//'):
                img_url = f"https:{img_url}"
            elif img_url.startswith('/'):
                img_url = f"{BASE_URL}{img_url}"
            elif not img_url.startswith('http'):
                continue
            
            img_info['url'] = img_url
        
        # Alt í…ìŠ¤íŠ¸ ì¶”ì¶œ
        alt_text = img.get('alt', '').strip()
        if alt_text:
            img_info['alt'] = alt_text
        
        # Title ì†ì„± ì¶”ì¶œ
        title_text = img.get('title', '').strip()
        if title_text:
            img_info['title'] = title_text
        
        # ì´ë¯¸ì§€ ì£¼ë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        parent = img.find_parent(['figure', 'div', 'p'])
        if parent:
            caption = parent.find(class_=re.compile(r'caption|figcaption|image.*caption', re.I))
            if caption:
                caption_text = caption.get_text(strip=True)
                if caption_text:
                    img_info['caption'] = caption_text
            
            img_text_parts = []
            prev_sibling = img.find_previous_sibling(['p', 'div', 'span'])
            if prev_sibling:
                prev_text = prev_sibling.get_text(strip=True)
                if prev_text and len(prev_text) < 200:
                    img_text_parts.append(prev_text)
            
            next_sibling = img.find_next_sibling(['p', 'div', 'span'])
            if next_sibling:
                next_text = next_sibling.get_text(strip=True)
                if next_text and len(next_text) < 200:
                    img_text_parts.append(next_text)
            
            if img_text_parts:
                img_info['context'] = ' '.join(img_text_parts)
        
        if img_info:
            images.append(img_info)
    
    return images


async def discover_all_articles(page, limit: Optional[int] = None) -> List[str]:
    """ëª¨ë“  ì•„í‹°í´ URL ë°œê²¬"""
    all_articles = set()
    
    try:
        logger.info("ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì¤‘...")
        await page.goto(f"{HELP_CENTER_BASE}", wait_until="networkidle", timeout=30000)
        await asyncio.sleep(2)
        
        page_source = await page.content()
        soup = BeautifulSoup(page_source, 'html.parser')
        
        # ì¹´í…Œê³ ë¦¬ ë§í¬ ì°¾ê¸°
        category_links = soup.find_all('a', href=re.compile(r'/hc/' + LOCALE + r'/categories/\d+'))
        categories = set()
        for link in category_links:
            href = link.get('href', '')
            if href:
                if href.startswith('/'):
                    full_url = f"{BASE_URL}{href}"
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue
                if '/categories/' in full_url:
                    categories.add(full_url)
        
        logger.info(f"ë°œê²¬ëœ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(categories)}")
        
        # ê° ì¹´í…Œê³ ë¦¬ì—ì„œ ì„¹ì…˜ ì°¾ê¸°
        all_sections = set()
        for category_url in categories:
            try:
                logger.info(f"ì¹´í…Œê³ ë¦¬ ì ‘ì†: {category_url}")
                await page.goto(category_url, wait_until="networkidle", timeout=30000)
                await asyncio.sleep(1)
                
                cat_soup = BeautifulSoup(await page.content(), 'html.parser')
                section_links = cat_soup.find_all('a', href=re.compile(r'/hc/' + LOCALE + r'/sections/\d+'))
                
                for link in section_links:
                    href = link.get('href', '')
                    if href:
                        if href.startswith('/'):
                            full_url = f"{BASE_URL}{href}"
                        elif href.startswith('http'):
                            full_url = href
                        else:
                            continue
                        if '/sections/' in full_url:
                            all_sections.add(full_url)
            except Exception as e:
                logger.warning(f"ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨ ({category_url}): {e}")
                continue
        
        logger.info(f"ë°œê²¬ëœ ì„¹ì…˜ ìˆ˜: {len(all_sections)}")
        
        # ê° ì„¹ì…˜ì—ì„œ ì•„í‹°í´ ì°¾ê¸°
        for section_url in all_sections:
            try:
                logger.info(f"ì„¹ì…˜ ì ‘ì†: {section_url}")
                await page.goto(section_url, wait_until="networkidle", timeout=30000)
                await asyncio.sleep(1)
                
                section_soup = BeautifulSoup(await page.content(), 'html.parser')
                article_links = section_soup.find_all('a', href=re.compile(r'/hc/' + LOCALE + r'/articles/\d+'))
                
                for link in article_links:
                    href = link.get('href', '')
                    if href:
                        if href.startswith('/'):
                            full_url = f"{BASE_URL}{href}"
                        elif href.startswith('http'):
                            full_url = href
                        else:
                            continue
                        if '/articles/' in full_url:
                            all_articles.add(full_url)
                            
                        if limit and len(all_articles) >= limit:
                            break
                
                if limit and len(all_articles) >= limit:
                    break
            except Exception as e:
                logger.warning(f"ì„¹ì…˜ ì²˜ë¦¬ ì‹¤íŒ¨ ({section_url}): {e}")
                continue
        
        # ë©”ì¸ í˜ì´ì§€ì—ì„œë„ ì§ì ‘ ì•„í‹°í´ ë§í¬ ì°¾ê¸°
        await page.goto(f"{HELP_CENTER_BASE}", wait_until="networkidle", timeout=30000)
        await asyncio.sleep(1)
        main_soup = BeautifulSoup(await page.content(), 'html.parser')
        main_article_links = main_soup.find_all('a', href=re.compile(r'/hc/' + LOCALE + r'/articles/\d+'))
        for link in main_article_links:
            href = link.get('href', '')
            if href:
                if href.startswith('/'):
                    full_url = f"{BASE_URL}{href}"
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue
                if '/articles/' in full_url:
                    all_articles.add(full_url)
        
        logger.info(f"ì´ ë°œê²¬ëœ ì•„í‹°í´ ìˆ˜: {len(all_articles)}")
        return list(all_articles)
        
    except Exception as e:
        logger.error(f"ì•„í‹°í´ ë°œê²¬ ì‹¤íŒ¨: {e}")
        return []


async def extract_article_content(page, article_url: str) -> Optional[Dict]:
    """ì•„í‹°í´ ë‚´ìš© ì¶”ì¶œ"""
    try:
        logger.info(f"ì•„í‹°í´ ì ‘ì†: {article_url}")
        await page.goto(article_url, wait_until="networkidle", timeout=30000)
        await asyncio.sleep(1)
        
        page_source = await page.content()
        soup = BeautifulSoup(page_source, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ
        title_elem = soup.find('h1') or soup.find(class_=re.compile(r'article.*title|title.*article', re.I))
        title = title_elem.get_text(strip=True) if title_elem else "ì œëª© ì—†ìŒ"
        
        # ì„¹ì…˜/ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ì¶œ
        section_name = None
        category_name = None
        
        # Breadcrumbì—ì„œ ì„¹ì…˜/ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ì¶œ
        breadcrumb = soup.find(class_=re.compile(r'breadcrumb|bread.*crumb', re.I))
        if breadcrumb:
            breadcrumb_links = breadcrumb.find_all('a')
            for link in breadcrumb_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                if '/sections/' in href and not section_name:
                    section_name = text
                elif '/categories/' in href and not category_name:
                    category_name = text
        
        # ë³¸ë¬¸ ì¶”ì¶œ
        body_elem = (
            soup.find(class_=re.compile(r'article.*body|body.*article', re.I)) or
            soup.find('article') or
            soup.find(id=re.compile(r'article.*content|content.*article', re.I))
        )
        
        images = []
        body_text = ""
        
        if body_elem:
            images = extract_images_from_element(body_elem)
            for tag in body_elem(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                tag.decompose()
            body_text = body_elem.get_text(separator='\n', strip=True)
        else:
            main_content = soup.find('main') or soup.find('div', class_=re.compile(r'content|main', re.I))
            if main_content:
                images = extract_images_from_element(main_content)
                for tag in main_content(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                    tag.decompose()
                body_text = main_content.get_text(separator='\n', strip=True)
            else:
                images = extract_images_from_element(soup)
                body_text = soup.get_text(separator='\n', strip=True)
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        lines = [line.strip() for line in body_text.split('\n') if line.strip()]
        clean_body = '\n'.join(lines)
        
        # ì´ë¯¸ì§€ ì„¤ëª… ì¶”ê°€
        image_descriptions = []
        for img in images:
            img_desc_parts = []
            if img.get('alt'):
                img_desc_parts.append(f"[ì´ë¯¸ì§€ ì„¤ëª…: {img['alt']}]")
            if img.get('caption'):
                img_desc_parts.append(f"[ì´ë¯¸ì§€ ìº¡ì…˜: {img['caption']}]")
            if img.get('context'):
                img_desc_parts.append(f"[ì´ë¯¸ì§€ ì£¼ë³€ ì„¤ëª…: {img['context']}]")
            if img_desc_parts:
                image_descriptions.append(' '.join(img_desc_parts))
        
        if image_descriptions:
            clean_body += "\n\n" + "\n".join(image_descriptions)
        
        # ì•„í‹°í´ ID ì¶”ì¶œ
        article_id_match = re.search(r'/articles/(\d+)', article_url)
        article_id = article_id_match.group(1) if article_id_match else None
        
        return {
            "url": article_url,
            "title": title,
            "body": clean_body,
            "article_id": article_id,
            "images": images,
            "section_name": section_name,
            "category_name": category_name,
            "full_text": f"ì œëª©: {title}\n\n{clean_body}"
        }
        
    except Exception as e:
        logger.error(f"ì•„í‹°í´ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ ({article_url}): {e}")
        return None


async def crawl_bithumb_faq(limit: Optional[int] = None, headless: bool = True):
    """ë¹—ì¸ FAQ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜"""
    if not PLAYWRIGHT_AVAILABLE:
        raise ImportError("Playwrightê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # ìƒëŒ€ ê²½ë¡œ import (airflow/scripts ë‚´ë¶€)
    from .mongodb_store import AirflowVectorStore
    
    logger.info("=" * 60)
    logger.info("ë¹—ì¸ FAQ í¬ë¡¤ë§ ì‹œì‘ (Playwright ì‚¬ìš©)")
    logger.info("=" * 60)
    
    # MongoDB ì—°ê²°
    logger.info("MongoDB Atlas ì—°ê²° ì¤‘...")
    vector_store = AirflowVectorStore()
    connected = await vector_store.connect()
    if not connected:
        raise ConnectionError("MongoDB ì—°ê²° ì‹¤íŒ¨")
    
    logger.info("âœ… MongoDB ì—°ê²° ì„±ê³µ!")
    
    # Playwright ë¸Œë¼ìš°ì € ì‹œì‘
    logger.info("ë¸Œë¼ìš°ì € ì‹œì‘ ì¤‘...")
    async with async_playwright() as p:
        try:
            browser = await p.chromium.launch(
                headless=headless,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                ]
            )
            
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                locale='ko-KR',
                timezone_id='Asia/Seoul',
            )
            
            await context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
                window.chrome = {
                    runtime: {}
                };
            """)
            
            page = await context.new_page()
            logger.info("âœ… ë¸Œë¼ìš°ì € ì‹œì‘ ì™„ë£Œ!")
            
            try:
                # ì•„í‹°í´ URL ë°œê²¬
                logger.info("ì•„í‹°í´ URL ë°œê²¬ ì¤‘...")
                article_urls = await discover_all_articles(page, limit=limit)
                
                if not article_urls:
                    logger.warning("ì•„í‹°í´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return
                
                if limit:
                    article_urls = article_urls[:limit]
                
                logger.info(f"ì´ {len(article_urls)}ê°œ ì•„í‹°í´ ë°œê²¬")
                logger.info("í¬ë¡¤ë§ ë° ë²¡í„° DB ì €ì¥ ì‹œì‘...")
                
                success_count = 0
                updated_count = 0
                skipped_count = 0
                fail_count = 0
                
                # ê° ì•„í‹°í´ ì²˜ë¦¬ ë° ì €ì¥
                for i, article_url in enumerate(article_urls, 1):
                    try:
                        logger.info(f"[{i}/{len(article_urls)}] í¬ë¡¤ë§ ì¤‘: {article_url}")
                        
                        # ì•„í‹°í´ ë‚´ìš© ì¶”ì¶œ
                        article_data = await extract_article_content(page, article_url)
                        
                        if not article_data or not article_data.get("body"):
                            fail_count += 1
                            logger.warning(f"ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {article_url}")
                            continue
                        
                        # ë²¡í„° DBì— ì €ì¥ (ë³€ê²½ ê°ì§€ í¬í•¨)
                        result = await vector_store.store_article(article_data)
                        
                        if result["status"] == "created":
                            success_count += 1
                            logger.info(f"âœ… ì‹ ê·œ ì €ì¥ ì™„ë£Œ: {article_data['title'][:40]}...")
                        elif result["status"] == "updated":
                            updated_count += 1
                            logger.info(f"ğŸ”„ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {article_data['title'][:40]}...")
                        elif result["status"] == "migrated":
                            updated_count += 1  # ë§ˆì´ê·¸ë ˆì´ì…˜ë„ ì—…ë°ì´íŠ¸ë¡œ ì¹´ìš´íŠ¸
                            logger.info(f"ğŸ”„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: {article_data['title'][:40]}... (content_hash ì¶”ê°€)")
                        elif result["status"] == "skipped":
                            skipped_count += 1
                            logger.info(f"â­ï¸  ë³€ê²½ì‚¬í•­ ì—†ìŒ (ìŠ¤í‚µ): {article_data['title'][:40]}...")
                        else:
                            fail_count += 1
                            logger.warning(f"ì €ì¥ ì‹¤íŒ¨: {article_url}")
                        
                        await asyncio.sleep(1)  # Rate limit ë°©ì§€
                        
                    except Exception as e:
                        fail_count += 1
                        logger.error(f"ì‹¤íŒ¨: {article_url} - {e}")
                        continue
                
                logger.info("=" * 60)
                logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
                logger.info(f"   ì‹ ê·œ ì €ì¥: {success_count}ê°œ")
                logger.info(f"   ì—…ë°ì´íŠ¸: {updated_count}ê°œ")
                logger.info(f"   ë³€ê²½ ì—†ìŒ (ìŠ¤í‚µ): {skipped_count}ê°œ")
                logger.info(f"   ì‹¤íŒ¨: {fail_count}ê°œ")
                logger.info(f"   ì´ ì²˜ë¦¬: {success_count + updated_count + skipped_count}ê°œ")
                logger.info("=" * 60)
                
            finally:
                await page.close()
                await context.close()
                await browser.close()
                logger.info("ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ")
        
        except Exception as e:
            logger.error(f"ë¸Œë¼ìš°ì € ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            raise
    
    # MongoDB ì—°ê²° í•´ì œ
    await vector_store.disconnect()
